{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some useful functions for computing the *tf-idf* for the data with arXiv abstracts.\n",
    "\n",
    "Note here the use of specific pandas function like *concat*, *value_counts* and *groupby* which make possible to speed up the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hugojulia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./arxiv_articles_simplified.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    A new simple proof of Stirling's formula via t...\n",
       "Name: summary, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[2:2, \"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naif_regex_tokenize(text):\n",
    "    \"\"\"\n",
    "    This is a very naif way of tokenize a text. Just using the\n",
    "    regular expression \"[a-z]\" that will match any single word\n",
    "    in lowercase.\n",
    "    Returns a list with all the tokens.\n",
    "    \"\"\"\n",
    "    p = re.compile(\"[a-z]+\")\n",
    "    return p.findall(text.lower())\n",
    "\n",
    "def compute_tf(d):\n",
    "    \"\"\"\n",
    "    Compute the tf for a given document d.\n",
    "    The formula used is \n",
    "    \n",
    "        tf(t, d) = 0.5 + 0.5 * (count(t, d)/max(count(t',d) for t' in d))\n",
    "    \n",
    "    This prevents bias in longer documents.\n",
    "    \"\"\"\n",
    "    terms = pd.Series(naif_regex_tokenize(d))\n",
    "    term_counts = terms.value_counts()\n",
    "    max_tc = max(term_counts)\n",
    "    return 0.5 + 0.5 * (term_counts / max_tc)\n",
    "\n",
    "def compute_idf(D):\n",
    "    \"\"\"\n",
    "    The input D is a list of pandas.Series\n",
    "    having as each element, the term frequency \n",
    "    computed by the function compute_tf.\n",
    "    \"\"\"\n",
    "    N = len(D)\n",
    "    all_terms = pd.concat(D)\n",
    "    nt = all_terms.index.value_counts() # The number of documents containing the term \"t\"\n",
    "    return np.log(N / nt)\n",
    "\n",
    "def compute_tf_idf_document(tf_document, idf):\n",
    "    \"\"\"Compute the tf-idf for each term in a document of the corpus\n",
    "\n",
    "    Keyword arguments:\n",
    "    tf_document -- list with the frequency of each term inside the document\n",
    "    idf -- the idf value for each term in the corpus\n",
    "    \"\"\"\n",
    "    return tf_document * np.array([idf[i] for i in tf_document.index])\n",
    "    \n",
    "def compute_tf_idf_corpus(D):\n",
    "    \"\"\"Compute the tf-idf for each term in a corpus\n",
    "\n",
    "    Keyword arguments:\n",
    "    D -- pandas Series containing a collection of documents in text format\n",
    "    \n",
    "    returns\n",
    "        list of pandas Series containing the tf-idf(t, d, D) for each term\n",
    "        inside each document of the corpus D\n",
    "    \"\"\"\n",
    "    term_freq = [compute_tf(d) for d in D]\n",
    "    idf = compute_idf(term_freq)\n",
    "    return [compute_tf_idf_document(d, idf) for d in term_freq]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        We show that every essentially countable orbit...\n",
      "1        Hans Grauert died in September of 2011. This a...\n",
      "2        A new simple proof of Stirling's formula via t...\n",
      "3        To each natural deformation quantization on a ...\n",
      "4        We show that finite Galois extensions with cyc...\n",
      "                               ...                        \n",
      "39606    Recently, a novel method for developing filter...\n",
      "39607    We consider the problem of numerically evaluat...\n",
      "39608    Ecological processes may exhibit memory to pas...\n",
      "39609    Complex networks are used to describe a broad ...\n",
      "39610    The Hamiltonian Monte Carlo method generates s...\n",
      "Name: summary, Length: 39611, dtype: object\n"
     ]
    }
   ],
   "source": [
    "s = data['summary']\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    We show that every essentially countable orbit...\n",
      "1    Hans Grauert died in September of 2011. This a...\n",
      "2    A new simple proof of Stirling's formula via t...\n",
      "3    To each natural deformation quantization on a ...\n",
      "4    We show that finite Galois extensions with cyc...\n",
      "5    We develop Nevanlinna's theory for a class of ...\n",
      "6    Let H, K be subgroups of G. We investigate the...\n",
      "7    We discuss the mathematician George Bruce Hals...\n",
      "8    A proof for the maximum modulus principle (in ...\n",
      "9    The aim of this paper is to give some applicat...\n",
      "Name: summary, dtype: object\n"
     ]
    }
   ],
   "source": [
    "D1 = data['summary'][:10]\n",
    "print(D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = compute_tf_idf_corpus(data.loc[:, \"summary\"]) # Sauvegarder avec pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we have the tf-idf values at each document. In order to select the *most important* terms (i.e. the terms with higher tf-idf values), we compute the **mean** of the tf-idf for each term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hans Grauert died in September of 2011. This article reviews his life in mathematics and recalls some detail his major accomplishments.\n"
     ]
    }
   ],
   "source": [
    "print(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fms    5.771334\n",
      "fms    5.819832\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "all_terms = pd.concat(tf_idf)\n",
    "print(all_terms['fms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a           0.070318\n",
      "aa          3.745126\n",
      "aaa         4.856940\n",
      "aaaaa       5.671533\n",
      "aaaattga    5.850634\n",
      "              ...   \n",
      "zyjhk       5.513991\n",
      "zymes       5.604809\n",
      "zynga       8.079447\n",
      "zynq        5.653551\n",
      "zz          6.805840\n",
      "Length: 63578, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "mean_tf_idf = all_terms.groupby(all_terms.index).mean()\n",
    "print(mean_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tf_idf = mean_tf_idf.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63578\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted_tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017889347956179486"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_tf_idf[\"the\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant c'est à vous\n",
    "\n",
    "Utiliser le code et les fonctions ci-dessus pour :\n",
    "\n",
    "1. Proposer un dictionnaire des termes (une liste de mots qui peuvent être \"informatives\"). Par exemple, un mot qui est présent dans plus de 10 documents différents et qui a un fort valeur de *tf-idf* peut etre \"informatif\". \n",
    "\n",
    "2. Utiliser ces termes pour calculer, pour chaque document, un vecteur *tf-idf*. Ce vecteur aura les valeurs du tf-idf de chaqu'un des termes. Par example, si la liste de termes est [\"float\", \"genetic\", \"circular\"] et qu'on a 4 document. On doit produire une matrice de 4 lignes et 3 colonnes :\n",
    "```\n",
    "0.1 5.8 9\n",
    "4.7 1.0 3\n",
    "8.0 2.4 6.0\n",
    "0.3 9.1 3.2\n",
    "```   \n",
    "Ici, chaque ligne contient les valeurs de *tf-idf* de [\"float\", \"genetic\", \"circular\"] (dans cet ordre).\n",
    "\n",
    "3. Normaliser les lignes de cette matrice. La norme 2 des vecteurs *tf-idf* représentés à chaque ligne doit être 1. Les étapes **2** et **3** font partie de ce qu'on appelle *feature extraction*. \n",
    "\n",
    "4. Executer l'exemple décrit [ici](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). Appliquer la même analyse sur notre jeu des données.\n",
    "\n",
    "5. Faire une implémentation de l'algorithme k-means. Voir les liens : https://en.wikipedia.org/wiki/K-means_clustering, https://en.wikipedia.org/wiki/K-means%2B%2B, https://fr.wikipedia.org/wiki/K-moyennes\n",
    "\n",
    "6. Implémenter une fonction permettant de trier les documents en fonction du résultat de l'algorithme k-means avec **k** groupes.\n",
    "\n",
    "7. Executer l'exemple décrit [ici](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py) puis appliquer la même analyse sur notre jeu des données\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['birational']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# fonction qui détermine si un mot est dans un texte (retourne un boolean)\n",
    "def appartient(texte, mot):\n",
    "    liste = naif_regex_tokenize(texte)\n",
    "    appartient = False\n",
    "    for i in liste:\n",
    "        if mot == i:\n",
    "            appartient = True\n",
    "            break\n",
    "    return appartient \n",
    "\n",
    "#fonction qui retourne une liste de mots qui apparaissent dans plus de 10 docs\n",
    "def new_termes(liste_mots, corpus):\n",
    "    new_list = []\n",
    "    liste_mots_filtered = []\n",
    "    \n",
    "    for j in liste_mots:\n",
    "        if j not in stopwords:\n",
    "            liste_mots_filtered.append(j)\n",
    "    \n",
    "    \n",
    "    for mot in liste_mots_filtered:\n",
    "        n = 0\n",
    "        for texte in corpus:\n",
    "            if appartient(texte, mot):\n",
    "                n += 1\n",
    "        if n >= 3:\n",
    "            new_list += [mot]         \n",
    "    return new_list\n",
    "\n",
    "#on prend les 500 mots avec le plus fort tf-idf et on regarde si ils apparaissent dans plus de 10 docs parmi les 1500 premiers documents du dataset\n",
    "informatifs = new_termes(sorted_tf_idf[:2000].index, data.loc[:1500, \"summary\"])\n",
    "\n",
    "print(informatifs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "1501\n"
     ]
    }
   ],
   "source": [
    "def vecteur_tf_idf (informatifs, tf_idf, corpus):\n",
    "    vecteur = np.zeros([len(corpus), len(informatifs)])\n",
    "    for doc, tfidf in enumerate(tf_idf):\n",
    "        for mot in tfidf.index:\n",
    "            if mot in informatifs:\n",
    "                vecteur[doc][informatifs.index(mot)]=tfidf[mot]\n",
    "    return vecteur\n",
    "      \n",
    "vecteur_tf_idf = vecteur_tf_idf(informatifs, tf_idf, data.loc[:1500, \"summary\"])\n",
    "print(vecteur_tf_idf[:5])\n",
    "print(len(vecteur_tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0.         0.         0.         0.53175633 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.46824367]\n",
      "[0.         0.         0.         0.53175633 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.46824367]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "def normalise (vecteur_tf_idf):\n",
    "    norme = vecteur_tf_idf\n",
    "    S = np.zeros(len(vecteur_tf_idf))\n",
    "    for i in range(0, vecteur_tf_idf.shape[0]):\n",
    "        for j in range(0, vecteur_tf_idf.shape[1]):\n",
    "            S[i] += vecteur_tf_idf[i][j]\n",
    "    for m in range(0, norme.shape[0]):\n",
    "        if S[m] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            norme[m] = np.true_divide(norme[m], S[m])  #np.true_divide divise toute la ligne de la matrice par la somme de ses éléments\n",
    "    return norme\n",
    "\n",
    "L = normalise(vecteur_tf_idf)\n",
    "\n",
    "for i in range(0, L.shape[0]):    #afficher toutes les lignes qui sont non nulles\n",
    "    for j in range(0, L.shape[1]):\n",
    "        if L[i][j] != 0:\n",
    "            print(L[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = normalise(vecteur_tf_idf)\n",
    "\n",
    "for i in range(len(S)):\n",
    "    if S[i] != 0:\n",
    "        print(S[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
